{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# model.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class QNetwork(nn.Module):\n",
    "  \"\"\"Actor (Policy) Model.\"\"\"\n",
    "  \n",
    "  def __init__(self, state_size, action_size, seed):\n",
    "    \"\"\"Initialize parameters and build model.\n",
    "    Params\n",
    "    ======\n",
    "      state_size (int): Dimension of each state\n",
    "      action_size (int): Dimension of each action\n",
    "      seed (int): Random seed\n",
    "    \"\"\"\n",
    "    super(QNetwork, self).__init__()\n",
    "    self.seed = torch.manual_seed(seed)\n",
    "    \"*** YOUR CODE HERE ***\"\n",
    "    \n",
    "  def forward(self, state):\n",
    "    \"\"\"Build a network that maps state -> action values.\"\"\"\n",
    "    pass\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# dqn_agent.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from collections import namedtuple, deque\n",
    "\n",
    "# from model import QNetwork\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "BUFFER_SIZE = int(1e5)\n",
    "BATCH_SIZE = 64\n",
    "GAMMA = 0.99\n",
    "TAU = 1e-3\n",
    "LR = 5e-4\n",
    "UPDATE_EVERY = 4\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "class Agent():\n",
    "  \"\"\"Interacts with and learns from the environment.\"\"\"\n",
    "  \n",
    "  def __init__(self, state_size, action_size, seed):\n",
    "    \"\"\"Initialize an Agent object.\n",
    "    Params\n",
    "    ======\n",
    "      state_size (int): dimension of each state\n",
    "      action_size (int): dimension of each action\n",
    "      seed (int): random seed\n",
    "    \"\"\"\n",
    "    self.state_size = state_size\n",
    "    self_action_size = action_size\n",
    "    self.seed = random.seed(seed)\n",
    "    \n",
    "    # Q-Network\n",
    "    self.qnetwork_local = QNetwork(state_size, action_size, seed).to(device)\n",
    "    self.qnetwork_target = QNetwork(state_size, action_size, seed).to(device)\n",
    "    self.optimizer = optim.Adam(self.qnetwork_local.parameters(), lr=LR)\n",
    "    \n",
    "    # Replay memory\n",
    "    self.memory = ReplayBuffer(action_size, BUFFER_SIZE, BATCH_SIZE, seed)\n",
    "    self.t_step = 0\n",
    "    \n",
    "  def step(self, state, action, reward, next_state, done):\n",
    "    # Save experience in replay memory\n",
    "    self.memory.add(state, action, reward, next_state, done)\n",
    "    \n",
    "    # Learn every UPDATE_EVERY time steps\n",
    "    self.t_step = (self.t_step + 1) % UPDATE_EVERY\n",
    "    if self.t_step == 0:\n",
    "      # If enough samples are available in memory, geet random subset and learn\n",
    "      if len(self.memory) > BATCH_SIZE:\n",
    "        experiences = self.memory.sample()\n",
    "        self.learn(experiences, GAMMA)\n",
    "        \n",
    "  def act(self, state, eps=0.):\n",
    "    \"\"\"Returns actions for given state as per current policy.\n",
    "    \n",
    "    Params\n",
    "    ======\n",
    "      state (array_like): current state\n",
    "      eps (float): epsilon, for epsilon-greedy actio selection\n",
    "    \"\"\"\n",
    "    state = torch.from_numpy(state).float().unsqueeze(0).to(device)\n",
    "    self.qnetwork_local.eval()\n",
    "    with torch.no_grad():\n",
    "      action_values = self.qnetwork_local(state)\n",
    "    self.qnetwork_local.train()\n",
    "    \n",
    "    if random.random() > eps:\n",
    "      return np.argmax(action_values.cpu().data.numpy())\n",
    "    else:\n",
    "      return random.choice(np.arange(self.action_size))\n",
    "    \n",
    "  def learn(self, experiences, gamma):\n",
    "    \"\"\"Update value parameters using give batch of experience tuples.\n",
    "    \n",
    "    Params\n",
    "    ======\n",
    "      experiences (Tuple[torch.Variable]): tuple of (s, a, r, s', done) tuples\n",
    "      gamma (float): discount factor\n",
    "    \"\"\"\n",
    "    states, actions, rewards, next_states, dones = experiences\n",
    "    \n",
    "    ## TODO: compute and minimize the loss\n",
    "    \"*** YOUR CODE HERE ***\"\n",
    "    \n",
    "    # -------------------- update target network ----------------------- #\n",
    "    self.soft_update(self.qnetwork_local, self.qnetwork_target, TAU)\n",
    "    \n",
    "  def soft_update(self, local_model, target_model, tau):\n",
    "    \"\"\"Soft update model parameters.\n",
    "    θ_target = τ*θ_local + (1 - τ)*θ_target\n",
    "    \n",
    "    \n",
    "    Params\n",
    "    ======\n",
    "      local_model (PyTorch model): weights will be copied from\n",
    "      target_model (PyTorch model): weights will be copied to\n",
    "      tau (float): interpolation parameter\n",
    "    \"\"\"\n",
    "    for target_param, local_param in zip(target_model.parameters(), local_model.parameters()):\n",
    "      target_param.data.copy_(tau*local_param.data + (1.0-tau)*target_param.data)\n",
    "\n",
    "      \n",
    "class ReplayBuffer:\n",
    "  \"\"\"Fixed-size buffer to store experience tuples.\"\"\"\n",
    "  \n",
    "  def __init__(self, action_size, buffer_size, batch_size, seed):\n",
    "    \"\"\"Initialize a ReplayBuffer object\n",
    "    \n",
    "    Params\n",
    "    ======\n",
    "      action_size (int): dimension of each action\n",
    "      buffer_size (int): maximum size of buffer\n",
    "      batch_size (int): size of each training batch\n",
    "      seed (int): random seed\n",
    "    \"\"\"\n",
    "    self.action_size = action_size\n",
    "    self.memory = deque(maxlen=buffer_size)\n",
    "    self.batch_size = batch_size\n",
    "    self.experience = namedtuple('Experience', field_names=['state', 'action', 'reward', 'next_state', 'done'])\n",
    "    self.seed = random.seed(seed)\n",
    "    \n",
    "  def add(self, state, action, reward, next_state, done):\n",
    "    \"\"\"Add a new expereince to memory.\"\"\"\n",
    "    e = self.experience(state, action, reward, next_state, done)\n",
    "    self.momory.append(e)\n",
    "    \n",
    "  def sample(self):\n",
    "    \"\"\"Randomly sample a batch of experience from memory.\"\"\"\n",
    "    experiences = random.sampe(self.memory, k=self.batch_size)\n",
    "    \n",
    "    states = torch.from_numpy(np.vstack([e.state for e in experiences if e is not None])).float().to(device)\n",
    "    actions = torch.from_numpy(np.vstack([e.action for e in experiences if e is not None])).float().to(device)\n",
    "    rewards = torch.from_numpy(np.vstack([e.reward for e in experiences if e is not None])).float().to(device)\n",
    "    next_states = torch.from_numpy(np.vstack([e.next_state for e in experiences if e is not None])).float().to(device)\n",
    "    dones = torch.from_numpy(np.vstack([e.done for e in experiences if e is not None]).astype(np.uint8)).float().to(device)\n",
    "    \n",
    "    return (states, actions, rewards, next_states, dones)\n",
    "  \n",
    "  def __len__(self):\n",
    "    \"\"\"Return the current size of internal memory.\"\"\"\n",
    "    return len(self.memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Q-Network (DQN)\n",
    "In this notebook, you will implement a DQN agent with OpenAI Gym's LunarLander-v2 environment\n",
    "\n",
    "### 1. Import the Necessary Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: box2d in /Users/hyunjaelee/anaconda/lib/python3.6/site-packages (2.3.2)\r\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "#!pip3 install box2d <- 터미널에서 실행.\n",
    "import random\n",
    "import torch\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 2. Instantiate the Environment and Agent\n",
    "Initialize the environment in the code cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State shape:  (8,)\n",
      "Number of actions:  4\n"
     ]
    }
   ],
   "source": [
    "#!pip install box2d box2d-kengz <- 터미널에서 실행.\n",
    "  \n",
    "env = gym.make('LunarLander-v2')\n",
    "env.seed(0)\n",
    "print('State shape: ', env.observation_space.shape)\n",
    "print('Number of actions: ', env.action_space.n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before running the next code cell, familiarize yourself with the code in **Step 2 and Step 3** of this notebook, along with the code in `dqn-agent.py` and `model.py`. Once you have an understanding of how the different files work together,\n",
    "- Define a neural network architecture in model.py that maps states to action values. This file is mostly empty - it's up to you to define your own deep Q-network!\n",
    "- Finish the learn method in the Agent class in dqn_agent.py The sampled batch of experience tuples is already provided for you; you need only use the local and target Q-networks to compute the loss, before taking a step towards minimizing the loss.\n",
    "\n",
    "Once yout have completed the code in dqn_agent.py and model.py, run the code cell below. (If you end up needing to make multiple changes and get unexpected behavior, please restart the kernel and run the cells from the beginning of the notebook!)\n",
    "\n",
    "You can find the solution files, along with saved model weights for a trained agent, in the solution/ folder. (Note that there are many ways to solve this exercise, and the \"solution\" is just one way of approaching the problem, to yield a trained agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
