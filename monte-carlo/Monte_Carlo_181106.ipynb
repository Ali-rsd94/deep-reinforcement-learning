{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Monte Carlo Methods\n",
    "In this notebook, you will write your own implementations of many Monte Carlo(MC) algorithms.\n",
    "While we have provided some starter code, you are welcome to erase these hints and write code from scratch\n",
    "\n",
    "## Part 0: Explore BlackjackEnv\n",
    "We begin by importing the necessary packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import gym\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "\n",
    "from plot_utils import plot_blackjack_values, plot_policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the code cell below to create an instance of the Blackjack environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "env = gym.make('Blackjack-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuple(Discrete(32), Discrete(11), Discrete(2))\n"
     ]
    }
   ],
   "source": [
    "print(env.observation_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discrete(2)\n"
     ]
    }
   ],
   "source": [
    "print(env.action_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6, 10, False)\n",
      "(16, 10, False)\n",
      "End game! Reward:  -1\n",
      "You lost :(\n",
      "\n",
      "(14, 10, True)\n",
      "(15, 10, True)\n",
      "(17, 10, True)\n",
      "(21, 10, True)\n",
      "(13, 10, False)\n",
      "End game! Reward:  -1.0\n",
      "You lost :(\n",
      "\n",
      "(14, 8, False)\n",
      "(17, 8, False)\n",
      "End game! Reward:  -1\n",
      "You lost :(\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i_episode in range(3):\n",
    "  state = env.reset()\n",
    "  while True:\n",
    "    print(state)\n",
    "    action = env.action_space.sample()\n",
    "    state, reward, done, info = env.step(action)\n",
    "    if done:\n",
    "      print('End game! Reward: ', reward)\n",
    "      print('You won :)\\n') if reward > 0 else print('You lost :(\\n')\n",
    "      break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: MC Prediction\n",
    "In this section, you will write your own implementation of MC predction (for estimating the action-value function).\n",
    "\n",
    "We will begin by investigating a policy where the player almost sticks if the sum of her cards exceeds 18. In particular, she select action STICK with 80% probability if the sum is greater than 18; and, if the sum is 18 or below, she selects action HIT with 80% probability. The function generate_episode_from_limit_stochastic samples an episode using this policy.\n",
    "\n",
    "It returns as output:\n",
    "* episode: This is a list of (state, action, reward) tuples (of tuples) and corresponds to $(S_0, A_0, R_1, \\ldots, S_{T-1}, A_{T-1}, R_{T})$, where $T$ is the final time step. In particular, `episode[i]` returns $(S_i, A_i, R_{i+1})$, and `episode[i][0]`, `episode[i][1]`, and `episode[i][2]` return $S_i$, $A_i$, and $R_{i+1}$, repectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_episode_from_limit_stochastic(bj_env):\n",
    "  episode = []\n",
    "  state = bj_env.reset()\n",
    "  while True:\n",
    "    probs = [0.8, 0.2] if state[0] > 18 else [0.2, 0.8]\n",
    "    action = np.random.choice(np.arange(2), p=probs)\n",
    "    next_state, reward, done, info = bj_env.step(action)\n",
    "    episode.append((state, action, reward))\n",
    "    state = next_state\n",
    "    if done:\n",
    "      break;\n",
    "  return episode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[((14, 10, False), 1, 0), ((21, 10, False), 0, 1.0)]\n",
      "[((11, 8, False), 1, 0), ((21, 8, False), 0, 1.0)]\n",
      "[((17, 8, False), 1, -1)]\n"
     ]
    }
   ],
   "source": [
    "for i in range(3):\n",
    "  print(generate_episode_from_limit_stochastic(env))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4 5] [1 2] [3 4 5] [1 2 3]\n"
     ]
    }
   ],
   "source": [
    "arr = np.array([1,2,3,4,5])\n",
    "arr2 = np.array([1,2,3,4,5,6])\n",
    "print(arr[3:], arr2[:-4], arr[2:], arr2[:-3])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def mc_prediction_q(env, num_episodes, generate_episode, gamma=1.0):\n",
    "  # initialize empty dictionaries of arrays\n",
    "  returns_sum = defaultdict(lambda: np.zeros(env.action_space.n))\n",
    "  N = defaultdict(lambda: np.zeros(env.action_space.n))\n",
    "  Q = defaultdict(lambda: np.zeros(env.action_space.n))\n",
    "  # loop over episodes\n",
    "  for i_episode in range(1, num_episodes+1):\n",
    "    # monitor progress\n",
    "    if i_episode % 1000 == 0:\n",
    "      print(\"\\rEpisode {}/{}.\".format(i_episode, num_episodes), end=\"\")\n",
    "      sys.stdout.flush()\n",
    "    # generate an episode\n",
    "    episode = generate_episode(env)\n",
    "    if i_episode % 1000 == 0:\n",
    "      print(episode[:2])\n",
    "    # obtain the states, actions, and rewards\n",
    "    states, actions, rewards = zip(*episode)\n",
    "    # prepare for discounting\n",
    "    discounts = np.array([gamma**i for i in range(len(rewards)+1)])\n",
    "    # update the sum of the returns, number of visits, and action-value\n",
    "    # function estimates for each state-action pair in the episode\n",
    "    for i, state in enumerate(states):\n",
    "      returns_sum[state][actions[i]] += sum(rewards[i:]*discounts[:-(1+i)])\n",
    "      N[state][actions[i]] += 1.0\n",
    "      Q[state][actions[i]] = returns_sum[state][actions[i]] / N[state][actions[i]]\n",
    "  return Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1000/50000.[((6, 8, False), 1, 0), ((10, 8, False), 1, 0)]\n",
      "Episode 2000/50000.[((14, 10, False), 1, 0), ((15, 10, False), 1, -1)]\n",
      "Episode 3000/50000.[((20, 10, False), 1, -1)]\n",
      "Episode 4000/50000.[((14, 7, False), 1, -1)]\n",
      "Episode 5000/50000.[((15, 10, False), 1, -1)]\n",
      "Episode 6000/50000.[((13, 10, False), 1, 0), ((21, 10, False), 0, 1.0)]\n",
      "Episode 7000/50000.[((17, 3, True), 1, 0), ((21, 3, True), 1, 0)]\n",
      "Episode 8000/50000.[((20, 9, False), 0, 1.0)]\n",
      "Episode 9000/50000.[((17, 6, False), 1, -1)]\n",
      "Episode 10000/50000.[((16, 10, False), 1, -1)]\n",
      "Episode 11000/50000.[((15, 6, False), 1, -1)]\n",
      "Episode 12000/50000.[((11, 10, False), 1, 0), ((21, 10, False), 0, 1.0)]\n",
      "Episode 13000/50000.[((14, 8, False), 0, -1.0)]\n",
      "Episode 14000/50000.[((10, 4, False), 1, 0), ((12, 4, False), 1, 0)]\n",
      "Episode 15000/50000.[((12, 7, False), 0, -1.0)]\n",
      "Episode 16000/50000.[((15, 2, False), 1, 0), ((20, 2, False), 0, -1.0)]\n",
      "Episode 17000/50000.[((12, 9, False), 1, -1)]\n",
      "Episode 18000/50000.[((19, 9, True), 1, 0), ((12, 9, False), 1, 0)]\n",
      "Episode 19000/50000.[((20, 2, True), 0, 1.0)]\n",
      "Episode 20000/50000.[((12, 10, False), 0, -1.0)]\n",
      "Episode 21000/50000.[((15, 7, False), 1, -1)]\n",
      "Episode 22000/50000.[((12, 5, False), 0, -1.0)]\n",
      "Episode 23000/50000.[((19, 10, False), 1, -1)]\n",
      "Episode 24000/50000.[((14, 1, False), 1, 0), ((15, 1, False), 0, -1.0)]\n",
      "Episode 25000/50000.[((21, 10, True), 0, 1.0)]\n",
      "Episode 26000/50000.[((7, 10, False), 1, 0), ((17, 10, False), 1, 0)]\n",
      "Episode 27000/50000.[((15, 5, True), 1, 0), ((16, 5, True), 1, 0)]\n",
      "Episode 28000/50000.[((21, 10, True), 0, 1.0)]\n",
      "Episode 29000/50000.[((7, 1, False), 1, 0), ((9, 1, False), 1, 0)]\n",
      "Episode 30000/50000.[((16, 3, True), 1, 0), ((17, 3, True), 1, 0)]\n",
      "Episode 31000/50000.[((17, 6, False), 1, 0), ((19, 6, False), 0, -1.0)]\n",
      "Episode 32000/50000.[((9, 10, False), 1, 0), ((11, 10, False), 1, 0)]\n",
      "Episode 33000/50000.[((10, 1, False), 1, 0), ((14, 1, False), 1, 0)]\n",
      "Episode 34000/50000.[((18, 10, False), 1, -1)]\n",
      "Episode 35000/50000.[((14, 10, False), 1, -1)]\n",
      "Episode 36000/50000.[((12, 10, False), 1, 0), ((19, 10, False), 0, 1.0)]\n",
      "Episode 37000/50000.[((19, 4, False), 0, -1.0)]\n",
      "Episode 38000/50000.[((10, 1, False), 0, -1.0)]\n",
      "Episode 39000/50000.[((19, 10, True), 0, -1.0)]\n",
      "Episode 40000/50000.[((12, 5, False), 0, -1.0)]\n",
      "Episode 41000/50000.[((16, 8, True), 1, 0), ((17, 8, True), 1, 0)]\n",
      "Episode 42000/50000.[((20, 10, False), 0, 1.0)]\n",
      "Episode 43000/50000.[((14, 8, False), 1, 0), ((15, 8, False), 1, 0)]\n",
      "Episode 44000/50000.[((21, 9, True), 0, 1.0)]\n",
      "Episode 45000/50000.[((13, 10, False), 1, 0), ((19, 10, False), 0, 0.0)]\n",
      "Episode 46000/50000.[((12, 9, False), 1, 0), ((18, 9, False), 1, -1)]\n",
      "Episode 47000/50000.[((13, 10, False), 1, 0), ((17, 10, False), 1, -1)]\n",
      "Episode 48000/50000.[((21, 7, True), 1, 0), ((21, 7, False), 1, -1)]\n",
      "Episode 49000/50000.[((16, 10, False), 1, -1)]\n",
      "Episode 50000/50000.[((16, 1, False), 0, -1.0)]\n"
     ]
    }
   ],
   "source": [
    "# obtain the action-value function\n",
    "Q = mc_prediction_q(env, 50000, generate_episode_from_limit_stochastic)\n",
    "\n",
    "# obtain the corresponding state-value functrion\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
